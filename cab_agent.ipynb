{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cab Driver Agent\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import product\n",
    "from collections import defaultdict, deque\n",
    "from datetime import datetime\n",
    "\n",
    "from cab_environment import CabDriverEnvironment\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Tracking\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tracking states initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_tracking_states():\n",
    "    state_action_pair = [((1, 0, 0), (1, 2)), ((2, 2, 2), (3, 4)),\n",
    "                         ((3, 5, 6), (3, 3)), ((5, 0, 0), (2, 3)),\n",
    "                         ((4, 14, 4), (4, 3))]\n",
    "\n",
    "    for st, ac in state_action_pair:\n",
    "        tracked_states[st] = defaultdict()\n",
    "        tracked_states[st][ac] = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### persist q-values corresponding to tracked states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tracking_states(driver):\n",
    "    for state in tracked_states.keys():\n",
    "        encoded_state = driver.cab_environment.state_encod_arch1(state)\n",
    "        q_predict = driver.model.predict(encoded_state)\n",
    "\n",
    "        for action in tracked_states[state].keys():\n",
    "            action_index = list(\n",
    "                filter(\n",
    "                    lambda x: action == driver.cab_environment.action_space[x],\n",
    "                    range(0, len(driver.cab_environment.action_space))))[0]\n",
    "            q_value = q_predict[0][action_index]\n",
    "            tracked_states[state][action].append(q_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon - Greedy Strategy\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon greedy policy is a way of selecting random actions with uniform distribution from a set of available actions. Using this policy either we can select random action with epsilon probability and we can select an action with 1-epsilon probability that gives maximum reward in given state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check epsilon decay over episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon = []\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.001\n",
    "\n",
    "# number of episodes for training : 10,000\n",
    "episodes = np.arange(0, 10000)\n",
    "\n",
    "for i in episodes:\n",
    "    epsilon.append(min_epsilon +\n",
    "                   (max_epsilon - min_epsilon) * np.exp(-0.0003 * i))\n",
    "\n",
    "plt.plot(episodes, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  formula to calculate epsilon : **(min_epsilon + (max_epsilon - min_epsilon) * np.exp(-0.0003 * i))**\n",
    "-  minimum epsilon : 0.001\n",
    "-  maximum epsilon : 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Agent Class\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Network Architecture\n",
    "---\n",
    "\n",
    "- input : encoded state i.e. each vector is a combination of locations + hours in a day + days in a week\n",
    "- output : q-values for all actions including (0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper parameters\n",
    "---\n",
    "\n",
    "- state_size : vector length of encoded states (number of neurons in input layer) : (36 -> 5 cities + 24 hours + 7 days)\n",
    "- action_size : vector length of predicted q_values for all actions : (21 -> (4*5) + 1  -> (m-1 * m) + 1) )\n",
    "- learning_rate\n",
    "- discount_factor\n",
    "- batch_size : batch size used in neural network for training\n",
    "- memory_length : replay memory buffer size\n",
    "- nn_epochs : number of epochs for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "                 state_size,\n",
    "                 action_size,\n",
    "                 learning_rate=0.09,\n",
    "                 gamma=0.95,\n",
    "                 batch_size=32,\n",
    "                 memory_length=2000,\n",
    "                 nn_epochs=1):\n",
    "\n",
    "        self.set_hyperparameters(state_size, action_size, learning_rate, gamma,\n",
    "                                 batch_size, memory_length, nn_epochs)\n",
    "\n",
    "        # environment object\n",
    "        self.cab_environment = CabDriverEnvironment()\n",
    "\n",
    "        # create main and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        # create a model directory\n",
    "        self.create_model_dir()\n",
    "\n",
    "    def set_hyperparameters(self, state_size, action_size, learning_rate,\n",
    "                            gamma, batch_size, memory_length, nn_epochs):\n",
    "        # initialize state-size and action-size which is define the input and output neurons\n",
    "        # of the neural network.\n",
    "        #\n",
    "        # state_size = number of locations(5) + hours in a day (24) + days in a week (7)\n",
    "        # action_size = total number of possible actions from a given state i.e. ((m-1) * m) + 1\n",
    "        #\n",
    "        # number of neurons in the input layer : state_size\n",
    "        # number of neurons in the output layer : action_size\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # define hyperparameters to be used for training the model\n",
    "        self.hyperparameters = {\n",
    "            'learning_rate': learning_rate,\n",
    "            'gamma': gamma,\n",
    "            'batch_size': batch_size,\n",
    "            'memory_length': memory_length,\n",
    "            'nn_epochs': nn_epochs\n",
    "        }\n",
    "\n",
    "        # replay memory for DQN\n",
    "        self.replay_memory = collections.deque(\n",
    "            maxlen=self.hyperparameters['memory_length'])\n",
    "\n",
    "    def create_model_dir(self):\n",
    "        lrate, gamma, bsize, mlength, epochs = self.hyperparameters\n",
    "        \n",
    "        self.model_dir = 'rl_cab_model_arch1_lrate_{0}_gamma_{1}_batch_size_{2}_memory_length_{3}'.format(\n",
    "            lrate, gamma, bsize, mlength)\n",
    "\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.mkdir(self.model_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\" Build neural network model\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "\n",
    "        # Input layer : 'state_size'\n",
    "        model.add(\n",
    "            Dense(32,\n",
    "                  input_dim=self.state_size,\n",
    "                  activation='relu',\n",
    "                  kernel_initializer='he_uniform'))\n",
    "\n",
    "        # Hidden Layers\n",
    "        model.add(Dense(32, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "\n",
    "        # Output Layer : 'action_size'\n",
    "        model.add(\n",
    "            Dense(self.action_size,\n",
    "                  activation='relu',\n",
    "                  kernel_initializer='he_uniform'))\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.hyperparameters['learning_rate']))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state, episode):\n",
    "        \"\"\" Choose an action for a given state and episode based on\n",
    "            Epsilon-Greedy strategy\n",
    "        \"\"\"\n",
    "        epsilon_min = 0.001\n",
    "        epsilon_max = 1.0\n",
    "        \n",
    "        # calculate decay factor for a given episode\n",
    "        epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-0.0003 * episode)\n",
    "\n",
    "        random_value = np.random.random()\n",
    "        \n",
    "        # fetch possible requests and their index for a given location\n",
    "        # number of requests per location are calculated based on a poisson distribution\n",
    "        possible_requests_index, possible_requests = self.cab_environment.get_requests_per_location(state)\n",
    "        \n",
    "        if random_value > epsilon and len(possible_requests_index) > 0:\n",
    "            # exploitation\n",
    "            \n",
    "            # predict q-values of all possible actions for a given state\n",
    "            encoded_state = self.cab_environment.state_encod_arch1(state)\n",
    "            predicted_q_values = self.model.predict(encoded_state)\n",
    "            \n",
    "            # filter q-values corresponding to selected requests for a given location\n",
    "            filter_q_values = [predicted_q_values[0][i] for i in possible_requests_index]\n",
    "            \n",
    "            # select index of an action corresponding to max Q-value\n",
    "            index_max_q_value = np.argmax(np.array(filter_q_values))\n",
    "\n",
    "            # select request with maximum q-value\n",
    "            selected_action = possible_requests[index_max_q_value]\n",
    "\n",
    "        else:\n",
    "            # exploration\n",
    "\n",
    "            # select a random request from possible request space\n",
    "            selected_action = random.choice(possible_requests)\n",
    "            \n",
    "        return selected_action\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state):\n",
    "        self.replay_memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\" Pick samples randomly from replay memory (with batch_size) and train the network\n",
    "        \"\"\"\n",
    "        # start training only when the number of samples in memory > batch size\n",
    "        if len(self.replay_memory) > self.hyperparameters['batch_size']:\n",
    "            \n",
    "            batch_size = self.hyperparameters['batch_size']\n",
    "\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.replay_memory, batch_size)\n",
    "\n",
    "            # initialize input state vector S\n",
    "            encoded_state_input = np.zeros((batch_size, self.state_size))\n",
    "\n",
    "            # initialize input state vector S'\n",
    "            encoded_next_state_input = np.zeros((batch_size, self.state_size))\n",
    "\n",
    "            action_list, reward_list = [], []\n",
    "\n",
    "            # for each element in the mini-batch, update input vectors\n",
    "            # with encoded state values\n",
    "            for i in range(batch_size):\n",
    "                state, action, reward, next_state = mini_batch[i]\n",
    "\n",
    "                encoded_state = self.cab_environment.state_encod_arch1(state)\n",
    "                encoded_state_input[i] = encoded_state\n",
    "\n",
    "                encoded_next_state = self.cab_environment.state_encod_arch1(\n",
    "                    next_state)\n",
    "                encoded_next_state_input[i] = encoded_next_state\n",
    "\n",
    "                action_list.append(action)\n",
    "                reward_list.append(reward)\n",
    "\n",
    "            # once the input matrices S and S' have been created, perform a feed-forward.\n",
    "            # this will generate q-values for all possible actions of a batch input\n",
    "            \n",
    "            # generate Q(s, a)\n",
    "            current_state_q_values = self.model.predict(encoded_state_input)\n",
    "            \n",
    "            # generate Q(s', a)\n",
    "            next_state_q_values = self.model.predict(encoded_next_state_input)\n",
    "            \n",
    "            # for each element in the batch, update respective q-values using the\n",
    "            # predicted and target q-values\n",
    "            for i in range(batch_size):\n",
    "                # each memory element is of form (s, a, s', r)\n",
    "                # output of NN for a given state generates q-values for all possible actions\n",
    "                # hence q-value update must be made to only that action 'a' \n",
    "                # out of all possible actions in the output\n",
    "\n",
    "                # given an action 'a', find the corresponding action index\n",
    "                performed_action = list(\n",
    "                    filter(\n",
    "                        lambda x: self.cab_environment.action_space[x] == action_list[i],\n",
    "                        range(0, len(self.cab_environment.action_space))))[0]\n",
    "                \n",
    "                # update q-value using formula (r + max Q(s',a))\n",
    "                current_state_q_values[i][performed_action] = reward_list[i] + (\n",
    "                    self.hyperparameters['gamma'] *\n",
    "                    np.max(next_state_q_values))\n",
    "            \n",
    "            # train the model with update Q-values\n",
    "            self.model.fit(encoded_state_input,\n",
    "                           current_state_q_values,\n",
    "                           batch_size=batch_size,\n",
    "                           epochs=self.hyperparameters['nn_epochs'],\n",
    "                           verbose=0)\n",
    "\n",
    "    def save(self):\n",
    "        f_name = self.model_dir + \"/\" + \"Q_Network_Model_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".model\"\n",
    "        self.model.save_weights(f_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tracking states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {(1, 0, 0): defaultdict(None, {(1, 2): []}),\n",
       "             (2, 2, 2): defaultdict(None, {(3, 4): []}),\n",
       "             (3, 5, 6): defaultdict(None, {(3, 3): []}),\n",
       "             (5, 0, 0): defaultdict(None, {(2, 3): []}),\n",
       "             (4, 14, 4): defaultdict(None, {(4, 3): []})})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracked_states = defaultdict()\n",
    "initialize_tracking_states()\n",
    "tracked_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tracking variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 2000\n",
    "\n",
    "checkpoint_model_save = 1000\n",
    "checkpoint_state_tracking = 1000\n",
    "checkpoint_print_episodes = 100\n",
    "\n",
    "month_end_cost_time = 30 * 24\n",
    "\n",
    "training_total_rewards_distribution = []\n",
    "training_total_ride_time_distribution = []\n",
    "training_total_time_step_distribution = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From G:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                1184      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 21)                693       \n",
      "=================================================================\n",
      "Total params: 2,933\n",
      "Trainable params: 2,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "driver_A = DQNAgent(state_size=36,\n",
    "                    action_size=21,\n",
    "                    learning_rate=0.01,\n",
    "                    gamma=0.85,\n",
    "                    batch_size=32,\n",
    "                    memory_length=2000,\n",
    "                    nn_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q - Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(driver):\n",
    "    start_time = time.time()\n",
    "\n",
    "    for episode in range(total_episodes):\n",
    "\n",
    "        # reset driver state at the beginning of each episode\n",
    "        current_state = driver.cab_environment.set_init_state()\n",
    "\n",
    "        # reset temporary tracking variables\n",
    "        has_reached_terminal_state = False\n",
    "        total_ride_time_per_episode = 0\n",
    "        total_rewards_per_episode = 0\n",
    "        time_step_per_episode = 0\n",
    "\n",
    "        while not has_reached_terminal_state:\n",
    "\n",
    "            # get the current action based on epsilon-greedy strategy\n",
    "            current_action = driver.get_action(current_state, episode)\n",
    "\n",
    "            # given the current state and action, calculate next state, reward and ride time\n",
    "            next_state, total_rewards, total_ride_time = driver.cab_environment.get_next_state(\n",
    "                current_state, current_action)\n",
    "\n",
    "            total_ride_time_per_episode += total_ride_time\n",
    "            total_rewards_per_episode += total_rewards\n",
    "\n",
    "            if total_ride_time_per_episode >= month_end_cost_time:\n",
    "                has_reached_terminal_state = True\n",
    "\n",
    "            # append current experience to replay buffer\n",
    "            driver.append_sample(current_state, current_action, reward, next_state)\n",
    "\n",
    "            # train the model\n",
    "            driver.train_model()\n",
    "\n",
    "            current_state = next_state\n",
    "            time_step_per_episode += 1\n",
    "\n",
    "        training_total_rewards_distribution.append(total_rewards_per_episode)\n",
    "        training_total_ride_time_distribution.append(total_ride_time_per_episode)\n",
    "        training_total_time_step_distribution.append(time_step_per_episode)\n",
    "        update\n",
    "        \n",
    "        if (episode + 1) % checkpoint_print_episodes == 0:\n",
    "            print(\n",
    "                \"Episode : %d, Avg Monthly Reward : %f, Avg Ride Time : %f, Avg Per Episode Time Step : %f\"\n",
    "                % (episode + 1,\n",
    "                   np.mean(training_total_rewards_distribution),\n",
    "                   np.mean(training_total_ride_time_distribution),\n",
    "                   np.mean(training_total_time_step_distribution)))\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Elapsed time :\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
