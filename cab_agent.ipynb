{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cab Driver Agent\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "from cab_environment import CabDriverEnvironment\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Tracking\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tracking states initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_tracking_states():\n",
    "    state_action_pair = [((1, 0, 0), (1, 2)), ((2, 2, 2), (3, 4)),\n",
    "                         ((3, 5, 6), (3, 3)), ((5, 0, 0), (2, 3)),\n",
    "                         ((4, 14, 4), (4, 3))]\n",
    "\n",
    "    for st, ac in state_action_pair:\n",
    "        tracked_states[st][ac] = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### persist q-values corresponding to tracked states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon - Greedy Strategy\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon greedy policy is a way of selecting random actions with uniform distribution from a set of available actions. Using this policy either we can select random action with epsilon probability and we can select an action with 1-epsilon probability that gives maximum reward in given state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check epsilon decay over episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon = []\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.001\n",
    "\n",
    "# number of episodes for training : 10,000\n",
    "episodes = np.arange(0, 10000)\n",
    "\n",
    "for i in episodes:\n",
    "    epsilon.append(min_epsilon +\n",
    "                   (max_epsilon - min_epsilon) * np.exp(-0.0003 * i))\n",
    "\n",
    "plt.plot(episodes, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  formula to calculate epsilon : **(min_epsilon + (max_epsilon - min_epsilon) * np.exp(-0.0003 * i))**\n",
    "-  minimum epsilon : 0.001\n",
    "-  maximum epsilon : 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "                 state_size,\n",
    "                 action_size,\n",
    "                 learning_rate=0.09,\n",
    "                 gamma=0.95,\n",
    "                 batch_size=32,\n",
    "                 memory_length=2000,\n",
    "                 nn_epochs=1):\n",
    "\n",
    "        self.set_hyperparameters(state_size, action_size, learning_rate, gamma,\n",
    "                                 batch_size, memory_length, nn_epochs)\n",
    "\n",
    "        # environment object\n",
    "        self.cab_environment = CabDriverEnvironment()\n",
    "\n",
    "        # create main and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        # create a model directory\n",
    "        self.create_model_dir()\n",
    "\n",
    "    def set_hyperparameters(self, state_size, action_size, learning_rate,\n",
    "                            gamma, batch_size, memory_length, nn_epochs):\n",
    "        \"\"\" Hyperparameters used in training sample\n",
    "        \"\"\"\n",
    "        # initialize state-size and action-size which is define the input and output neurons\n",
    "        # of the neural network.\n",
    "        #\n",
    "        # state_size = number of locations(5) + hours in a day (24) + days in a week (7)\n",
    "        # action_size = total number of possible actions from a given state i.e. ((m-1) * m) + 1\n",
    "        #\n",
    "        # number of neurons in the input layer : state_size\n",
    "        # number of neurons in the output layer : action_size\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # define hyperparameters to be used for training the model\n",
    "        self.hyperparameters = {\n",
    "            'learning_rate': learning_rate,\n",
    "            'gamma': gamma,\n",
    "            'batch_size': batch_size,\n",
    "            'memory_length': memory_length,\n",
    "            'nn_epochs': nn_epochs\n",
    "        }\n",
    "\n",
    "        # replay memory for DQN\n",
    "        self.replay_memory = collections.deque(\n",
    "            maxlen=self.hyperparameters['memory_length'])\n",
    "\n",
    "    def create_model_dir(self):\n",
    "        \"\"\" Create a folder to store trained model\n",
    "        \"\"\"\n",
    "        learning_rate, gamma, batch_size, memory_length = self.hyperparameters\n",
    "\n",
    "        self.model_dir = 'rl_cab_model_arch1_lrate_{0}_gamma_{1}_batch_size_{2}_memory_length_{3}'.format(\n",
    "            learning_rate, gamma, batch_size, memory_length)\n",
    "\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.mkdir(self.model_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\" Define neural network\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "\n",
    "        # Input layer : 'state_size'\n",
    "        model.add(\n",
    "            Dense(32,\n",
    "                  input_dim=self.state_size,\n",
    "                  activation='relu',\n",
    "                  kernel_initializer='he_uniform'))\n",
    "\n",
    "        # Hidden Layers\n",
    "        model.add(Dense(32, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "\n",
    "        # Output Layer : 'action_size'\n",
    "        model.add(\n",
    "            Dense(self.action_size,\n",
    "                  activation='relu',\n",
    "                  kernel_initializer='he_uniform'))\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state, episode):\n",
    "        \"\"\" Choose an action for a given state and episode based on\n",
    "            Epsilon-Greedy strategy\n",
    "        \"\"\"\n",
    "        # calculate decay factor for a given episode\n",
    "        epsilon = self.epsilon_min + (\n",
    "            self.epsilon_max - self.epsilon_min) * np.exp(-0.0003 * episode)\n",
    "\n",
    "        random_value = np.random.random()\n",
    "\n",
    "        # fetch possible requests and their index for a given location\n",
    "        # number of requests per location are calculated based on a poisson distribution\n",
    "        request_index, possible_requests = self.cab_environment.get_requests_per_location(state)\n",
    "\n",
    "        if random_value > epsilon:\n",
    "            # exploitation\n",
    "\n",
    "            # predict q-values of all possible actions for a given state\n",
    "            encoded_state = self.cab_environment.state_encod_arch1(state)\n",
    "            predicted_q_values = self.model.predict(encoded_state)\n",
    "\n",
    "            # filter q-values corresponding to selected requests for a given location\n",
    "            filter_q_values = [predicted_q_values[0][i] for i in request_index]\n",
    "\n",
    "            # select index of an action corresponding to max Q-value\n",
    "            index_max_q_value = np.argmax(np.array(filter_q_values))\n",
    "\n",
    "            # select request with maximum q-value\n",
    "            selected_action = possible_requests[index_max_q_value]\n",
    "\n",
    "        else:\n",
    "            # exploration\n",
    "\n",
    "            # select a random request from possible request space\n",
    "            selected_action = random.choice(possible_requests)\n",
    "\n",
    "        return selected_action\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state):\n",
    "        \"\"\" Insert sample into replay memory buffer\n",
    "        \"\"\"\n",
    "        self.replay_memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def save_model(self):\n",
    "        file_name = self.model_dir + \"/\" + \"dqn_model_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".model\"\n",
    "        self.model.save_weights(file_name)\n",
    "        \n",
    "    def train_model(self):\n",
    "        \"\"\" Pick samples randomly from replay memory (with batch_size) and train the network\n",
    "        \"\"\"\n",
    "        # start training only when the number of samples in memory > batch size\n",
    "        if len(self.replay_memory) > self.hyperparameters['batch_size']:\n",
    "            batch_size = self.hyperparameters['batch_size']\n",
    "\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "            # initialize input state vector S\n",
    "            encoded_state_input = np.zeros((batch_size, self.state_size))\n",
    "\n",
    "            # initialize input state vector S'\n",
    "            encoded_next_state_input = np.zeros((batch_size, self.state_size))\n",
    "\n",
    "            action_list, reward_list = [], []\n",
    "\n",
    "            # for each element in the mini-batch, update input vectors\n",
    "            # with encoded state values\n",
    "            for i in range(batch_size):\n",
    "                state, action, reward, next_state = mini_batch[i]\n",
    "\n",
    "                encoded_state = self.cab_environment.state_encod_arch1(state)\n",
    "                encoded_state_input[i] = encoded_state\n",
    "\n",
    "                encoded_next_state = self.cab_environment.state_encod_arch1(\n",
    "                    next_state)\n",
    "                encoded_next_state_input[i] = encoded_next_state\n",
    "\n",
    "                action_list.append(action)\n",
    "                reward_list.append(reward)\n",
    "\n",
    "            # once the input matrices S and S' have been created, perform a feed-forward.\n",
    "            # this will generate q-values for all possible actions of a batch input\n",
    "\n",
    "            # generate Q(s, a)\n",
    "            current_state_q_values = self.model.predict(encoded_state_input)\n",
    "\n",
    "            # generate Q(s', a)\n",
    "            next_state_q_values = self.model.predict(encoded_next_state_input)\n",
    "\n",
    "            # for each element in the batch, update respective q-values using the\n",
    "            # formula (r + maxQ(s',a))\n",
    "            for i in range(batch_size):\n",
    "                action_idx = list(\n",
    "                    filter(\n",
    "                        lambda x: self.cab_environment.action_space[x] == action_list[i],\n",
    "                        range(0, len(self.cab_environment.action_space))))[0]\n",
    "                \n",
    "                # update q-values based on current model output\n",
    "                current_state_q_values[i][action_idx] = reward_list[i] + (\n",
    "                    self.hyperparameters['gamma'] *\n",
    "                    np.max(next_state_q_values))\n",
    "            \n",
    "            # train the model with update Q-values\n",
    "            self.model.fit(encoded_state_input,\n",
    "                           current_state_q_values,\n",
    "                           batch_size=batch_size,\n",
    "                           epochs=self.hyperparameters['nn_epochs'],\n",
    "                           verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
